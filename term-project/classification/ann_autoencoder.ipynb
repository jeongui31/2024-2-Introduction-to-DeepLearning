{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[모델 오토인코더를 활용한 ANN]\n",
    "1. Autoencoder를 사용해 데이터를 효율적으로 압축하고 특징을 추출한 후, 그 결과를 ANN으로 전달해 최종 분류를 수행\n",
    "- 은닉층 1개의 인코더와 디코더, ANN\n",
    "- 절단 정규 분포 (tf.truncated_normal), ReLU 활성화 함수 사용\n",
    "- 선형 회귀 모델 y=W*x+b\n",
    "2. 손실함수 : cross-entropy (softmax 함수 사용)\n",
    "3. 옵티마이저 : Gradient Descent Optimizer (learning rate=0.3)\n",
    "4. 학습 epoch=30\n",
    "- batch size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "def load_csv(file_path):\n",
    "\n",
    "    # CSV 파일 로드\n",
    "    data = np.loadtxt(file_path, delimiter=',', skiprows=1)\n",
    "    \n",
    "    # 입력 변수와 목표 변수 분리 ('fetal_health'를 목표 변수로 설정)\n",
    "    x_data = data[:, :-1]  # 마지막 열 제외 (입력 변수)\n",
    "    y_data = data[:, -1].astype(int)  # 마지막 열 (목표 변수)\n",
    "    \n",
    "    # 목표 변수를 one-hot 인코딩으로 변환\n",
    "    with tf.Session() as sess:\n",
    "        y_data = sess.run(tf.one_hot(y_data - 1, depth=3))\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "def split_data(x_data, y_data):\n",
    "    # 첫 200개의 데이터를 테스트 데이터로 사용\n",
    "    x_test = x_data[:200]\n",
    "    y_test = y_data[:200]\n",
    "    \n",
    "    # 나머지 데이터를 학습 및 검증 데이터로 사용\n",
    "    x_rest = x_data[200:]\n",
    "    y_rest = y_data[200:]\n",
    "    \n",
    "    # 학습 데이터와 검증 데이터로 분할 예시 (전체의 약 12.5%를 검증 데이터로 사용)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_rest, y_rest, test_size=0.125, random_state=42)\n",
    "\n",
    "    # 표준화 (스케일링)\n",
    "    sc = StandardScaler()\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    x_val = sc.transform(x_val)\n",
    "    x_test = sc.transform(x_test)\n",
    "    \n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = load_csv('fetal_health.csv')\n",
    "x_train, x_val, x_test, y_train, y_val, y_test = split_data(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.3\n",
    "num_epochs = 30      # 학습횟수\n",
    "batch_size = 32    # 배치개수\n",
    "display_step = 10    # 손실함수 출력 주기\n",
    "input_size = x_train.shape[1]    # 21\n",
    "hidden1_size = 128  # 첫 번째 은닉층의 크기\n",
    "hidden2_size = 64  # 두 번째 은닉층의 크기\n",
    "output_size = y_train.shape[1]   # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, input_size])\n",
    "y = tf.placeholder(tf.float32, shape=[None, output_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN 모델 구축 함수\n",
    "def build_ANN(encoded_input):\n",
    "    encoded_size = hidden1_size  # 인코딩된 출력 크기와 일치하도록 설정\n",
    "    hidden2_size = 64  # 두 번째 은닉층의 크기\n",
    "    output_size = y_train.shape[1]  # 클래스 수\n",
    "\n",
    "    # 첫 번째 은닉층\n",
    "    W1 = tf.Variable(tf.truncated_normal([encoded_size, hidden2_size], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.constant(0.1, shape=[hidden2_size]))\n",
    "    H1_output = tf.nn.relu(tf.matmul(encoded_input, W1) + b1)\n",
    "\n",
    "    # 출력층\n",
    "    W_output = tf.Variable(tf.truncated_normal([hidden2_size, output_size], stddev=0.1))\n",
    "    b_output = tf.Variable(tf.constant(0.1, shape=[output_size]))\n",
    "    logits = tf.matmul(H1_output, W_output) + b_output\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오토인코더 구축 함수\n",
    "def build_autoencoder(x):\n",
    "    # 인코더: 입력을 압축\n",
    "    W_enc = tf.Variable(tf.truncated_normal([input_size, hidden1_size], stddev=0.1))\n",
    "    b_enc = tf.Variable(tf.constant(0.1, shape=[hidden1_size]))\n",
    "    encoded = tf.nn.relu(tf.matmul(x, W_enc) + b_enc)\n",
    "\n",
    "    # 디코더: 압축된 데이터를 복원\n",
    "    W_dec = tf.Variable(tf.truncated_normal([hidden1_size, input_size], stddev=0.1))\n",
    "    b_dec = tf.Variable(tf.constant(0.1, shape=[input_size]))\n",
    "    decoded = tf.nn.relu(tf.matmul(encoded, W_dec) + b_dec)\n",
    "    \n",
    "    return encoded, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오토인코더로 입력을 인코딩한 후 Softmax 분류기로 전달\n",
    "encoded_data, reconstructed_data = build_autoencoder(x)\n",
    "logits = build_ANN(encoded_data)  # 인코더의 출력값을 ANN에 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수 및 최적화 설정\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "반복(Epoch): 1, 손실 함수(Loss): 0.385649\n",
      "검증 정확도(Validation Accuracy): 0.887967\n",
      "반복(Epoch): 11, 손실 함수(Loss): 0.143373\n",
      "검증 정확도(Validation Accuracy): 0.900415\n",
      "반복(Epoch): 21, 손실 함수(Loss): 0.091147\n",
      "검증 정확도(Validation Accuracy): 0.912863\n",
      "\n",
      "최종 테스트 정확도(Test Accuracy): 0.865000\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # 학습 루프\n",
    "    total_batch = len(x_train) // batch_size\n",
    "    for epoch in range(num_epochs):\n",
    "        average_loss = 0\n",
    "        for i in range(total_batch):\n",
    "            # 미니배치 데이터 준비\n",
    "            start = i * batch_size\n",
    "            end = start + batch_size\n",
    "            batch_x, batch_y = x_train[start:end], y_train[start:end]\n",
    "            \n",
    "            # 학습 수행 및 손실 계산\n",
    "            _, current_loss = sess.run([train_step, loss], feed_dict={x: batch_x, y: batch_y})\n",
    "            average_loss += current_loss / total_batch\n",
    "        \n",
    "        # 매 epoch마다 손실 및 검증 정확도 출력\n",
    "        if epoch % display_step == 0:\n",
    "            # 학습 데이터 손실\n",
    "            print(\"반복(Epoch): %d, 손실 함수(Loss): %f\" % ((epoch+1), average_loss))\n",
    "            \n",
    "            # 검증 데이터에 대한 정확도 계산\n",
    "            correct_prediction_val = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "            accuracy_val = tf.reduce_mean(tf.cast(correct_prediction_val, \"float\"))\n",
    "            print(\"검증 정확도(Validation Accuracy): %f\" % accuracy_val.eval(feed_dict={x: x_val, y: y_val}))\n",
    "    \n",
    "    # 테스트 데이터에 대한 최종 정확도 계산\n",
    "    correct_prediction_test = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy_test = tf.reduce_mean(tf.cast(correct_prediction_test, \"float\"))\n",
    "    print(\"\\n최종 테스트 정확도(Test Accuracy): %f\" % (accuracy_test.eval(feed_dict={x: x_test, y: y_test})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
