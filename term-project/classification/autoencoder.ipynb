{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[모델 Autoencoder와 softmax classifier를 결합]\n",
    "1. Autoencoder를 사용해 데이터를 효율적으로 압축하고 특징을 추출한 후, 그 결과를 ANN으로 전달해 최종 분류를 수행\n",
    "- 은닉층 3개로 구성\n",
    "- 가우시안 분포(정규분포)에서 임의의 값 추출, sigmoid 활성화 함수 사용\n",
    "- softmax 분류기\n",
    "2. 손실함수 : MSE & cross-entropy\n",
    "3. 옵티마이저\n",
    "- RMSProp Optimizer (learning rate=0.01)\n",
    "- Gradient Descent Optimizer (learning rate=0.1)\n",
    "4. 학습 epoch=40\n",
    "- batch size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "def load_csv(file_path):\n",
    "\n",
    "    # CSV 파일 로드\n",
    "    data = np.loadtxt(file_path, delimiter=',', skiprows=1)\n",
    "    \n",
    "    # 입력 변수와 목표 변수 분리 ('fetal_health'를 목표 변수로 설정)\n",
    "    x_data = data[:, :-1]  # 마지막 열 제외 (입력 변수)\n",
    "    y_data = data[:, -1].astype(int)  # 마지막 열 (목표 변수)\n",
    "    \n",
    "    # 목표 변수를 one-hot 인코딩으로 변환\n",
    "    with tf.Session() as sess:\n",
    "        y_data = sess.run(tf.one_hot(y_data - 1, depth=3))\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "def split_data(x_data, y_data):\n",
    "    # 첫 200개의 데이터를 테스트 데이터로 사용\n",
    "    x_test = x_data[:200]\n",
    "    y_test = y_data[:200]\n",
    "    \n",
    "    # 나머지 데이터를 학습 데이터로 사용\n",
    "    x_train = x_data[200:]\n",
    "    y_train = y_data[200:]\n",
    "    \n",
    "    # 표준화 (스케일링)\n",
    "    sc = StandardScaler()\n",
    "    x_train = sc.fit_transform(x_train)\n",
    "    x_test = sc.transform(x_test)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data, y_data = load_csv('fetal_health.csv')\n",
    "x_train, x_test, y_train, y_test = split_data(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_RMSProp = 0.01\n",
    "learning_rate_GradientDescent = 0.1\n",
    "num_epochs = 40\n",
    "batch_size = 32\n",
    "display_step = 10\n",
    "input_size = x_train.shape[1]\n",
    "hidden1_size = 64\n",
    "hidden2_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, input_size])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_autoencoder(x):\n",
    "  # 인코딩(Encoding)\n",
    "  Wh_1 = tf.Variable(tf.random_normal([input_size, hidden1_size]))   \n",
    "  bh_1 = tf.Variable(tf.random_normal([hidden1_size]))\n",
    "  H1_output = tf.nn.sigmoid(tf.matmul(x, Wh_1) +bh_1)\n",
    "  Wh_2 = tf.Variable(tf.random_normal([hidden1_size, hidden2_size]))\n",
    "  bh_2 = tf.Variable(tf.random_normal([hidden2_size]))\n",
    "  H2_output = tf.nn.sigmoid(tf.matmul(H1_output, Wh_2) +bh_2)\n",
    "  #디코딩(Decoding)\n",
    "  Wh_3 = tf.Variable(tf.random_normal([hidden2_size, hidden1_size]))\n",
    "  bh_3 = tf.Variable(tf.random_normal([hidden1_size]))\n",
    "  H3_output = tf.nn.sigmoid(tf.matmul(H2_output, Wh_3) +bh_3)\n",
    "  Wo = tf.Variable(tf.random_normal([hidden1_size, input_size]))\n",
    "  bo = tf.Variable(tf.random_normal([input_size]))\n",
    "  X_reconstructed = tf.nn.sigmoid(tf.matmul(H3_output,Wo) + bo)\n",
    "  \n",
    "  return X_reconstructed, H2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_softmax_classifier(x):\n",
    "  W_softmax = tf.Variable(tf.zeros([hidden2_size, 3]))\n",
    "  b_softmax = tf.Variable(tf.zeros([3]))\n",
    "  y_pred = tf.nn.softmax(tf.matmul(x, W_softmax) + b_softmax)\n",
    "\n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, extracted_features = build_autoencoder(x) \n",
    "y_true = x\n",
    "y_pred_softmax = build_softmax_classifier(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Pre-Training\n",
    "pretraining_loss = tf.reduce_mean(tf.pow(y_true - y_pred, 2))# MSE 손실 함수\n",
    "pretraining_train_step = tf.train.RMSPropOptimizer(learning_rate_RMSProp).minimize(pretraining_loss)\n",
    "# 2. Fine-Tuning \n",
    "finetuning_loss = tf.reduce_mean(-tf.reduce_sum(y * tf.log(y_pred_softmax), reduction_indices=[1]))# cross-entropy 손실 함수\n",
    "finetuning_train_step = tf.train.GradientDescentOptimizer(learning_rate_GradientDescent).minimize(finetuning_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, pretraining_loss: 1.032140\n",
      "Epoch: 10, pretraining_loss: 0.385775\n",
      "Epoch: 20, pretraining_loss: 0.377588\n",
      "Epoch: 30, pretraining_loss: 0.373338\n",
      "\n",
      "Epoch: 0, finetuning_loss: 0.267359\n",
      "Epoch: 10, finetuning_loss: 0.404640\n",
      "Epoch: 20, finetuning_loss: 0.443897\n",
      "Epoch: 30, finetuning_loss: 0.432973\n",
      "\n",
      "정확도(오토인코더+Softmax 분류기): 0.790000\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "  sess.run(tf.global_variables_initializer())\n",
    "\n",
    "  # 전체 배치 개수\n",
    "  total_batch = len(x_train) // batch_size\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    # 모든 배치들에 대해서 최적화를 수행\n",
    "    for i in range(total_batch):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_x, batch_y = x_train[start:end], y_train[start:end]\n",
    "        _, pretraining_loss_print = sess.run([pretraining_train_step, pretraining_loss], feed_dict={x: batch_x})\n",
    "    # 지정된 epoch마다 학습결과를 출력\n",
    "    if epoch % display_step == 0:\n",
    "      print(\"Epoch: %d, pretraining_loss: %f\" % (epoch, pretraining_loss_print))\n",
    "  print('')\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    # 모든 배치들에 대해서 최적화를 수행\n",
    "    for i in range(total_batch):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        batch_x, batch_y = x_train[start:end], y_train[start:end]\n",
    "        _, finetuning_loss_print = sess.run([finetuning_train_step, finetuning_loss], feed_dict={x: batch_x,  y: batch_y})\n",
    "    # 지정된 epoch마다 학습결과를 출력\n",
    "    if epoch % display_step == 0:\n",
    "      print(\"Epoch: %d, finetuning_loss: %f\" % (epoch, finetuning_loss_print))\n",
    "\n",
    "  # 오토인코더+Softmax 분류기 모델의 정확도를 출력\n",
    "  correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_pred_softmax,1))\n",
    "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "  print(\"\\n정확도(오토인코더+Softmax 분류기): %f\" % sess.run(accuracy, feed_dict={x: x_test, y: y_test}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro-DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
